# ğŸ“˜ Kafka Streams & Spring Cloud Stream - TP1

## ğŸ§© Introduction
Ce projet dÃ©montre comment intÃ©grer **Spring Cloud Stream** avec **Kafka** afin de produire, consommer et traiter en temps rÃ©el des Ã©vÃ©nements de type `PageEvent`.  
Lâ€™objectif est de comprendre le fonctionnement dâ€™un pipeline de donnÃ©es rÃ©actif avec Kafka Streams.

---

## âš™ï¸ 1. CrÃ©ation du Consumer simple

### ğŸ¯ Objectif
Configurer un **consumer Kafka** capable de lire les messages envoyÃ©s sur un topic et dâ€™afficher leur contenu.

### ğŸªœ Ã‰tapes
1. CrÃ©er un consumer sur le mÃªme topic que le producer.
2. Envoyer un message `"hello"` dans le topic.
3. VÃ©rifier que le consumer reÃ§oit bien le message dans la console.

ğŸ“¸ **Capture dâ€™Ã©cran â€“ Consumer recevant le message :**
> ![Consumer screenshot](images/image2.png)

---

## ğŸ§± 2. CrÃ©ation de la classe `PageEvent`

### ğŸ¯ Objectif
CrÃ©er un modÃ¨le dâ€™Ã©vÃ©nement reprÃ©sentant une page visitÃ©e, un utilisateur, une date et une durÃ©e.

### ğŸ’» Code
```java
package org.example.kafkastreamsspringcloudstreamtp1.events;

import java.util.Date;

public record PageEvent(String name, String user, Date date, long duration) {}

````

Cette classe sera utilisÃ©e pour Ã©changer des objets entre producer et consumer.

---

## ğŸš€ 3. CrÃ©ation du contrÃ´leur `PageEventController`

### ğŸ¯ Objectif

Publier un objet `PageEvent` dans un topic Kafka Ã  lâ€™aide du **StreamBridge** de Spring Cloud Stream.

### ğŸ’» Code

```java
package org.example.kafkastreamsspringcloudstreamtp1.controllers;

import org.example.kafkastreamsspringcloudstreamtp1.events.PageEvent;
import org.springframework.cloud.stream.function.StreamBridge;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;
import java.util.Date;
import java.util.Random;

@RestController
public class PageEventController {
    private final StreamBridge streamBridge;

    public PageEventController(StreamBridge streamBridge) {
        this.streamBridge = streamBridge;
    }

    @GetMapping("/publish")
    public PageEvent send(String name, String topic) {
        PageEvent event = new PageEvent(
                name,
                Math.random() > 0.5 ? "U1" : "U2",
                new Date(),
                10 + new Random().nextInt(1000)
        );
        streamBridge.send(topic, event);
        return event;
    }
}
```

Ce contrÃ´leur permet de publier un Ã©vÃ©nement via lâ€™URL :
`http://localhost:8080/publish?name=page1&topic=T1`

ğŸ“¸ **Capture dâ€™Ã©cran â€“ Publication dans T1 :**

> ![Publication screenshot](images/image5.png)

---

## ğŸ›°ï¸ 4. CrÃ©ation du Consumer `PageEventHandler`

### ğŸ¯ Objectif

CrÃ©er un bean `Consumer` qui lit les objets `PageEvent` depuis le topic Kafka et affiche leur contenu.

### ğŸ’» Code

```java
package org.example.kafkastreamsspringcloudstreamtp1.hanndlers;

import org.example.kafkastreamsspringcloudstreamtp1.events.PageEvent;
import org.springframework.context.annotation.Bean;
import org.springframework.stereotype.Component;
import java.util.function.Consumer;

@Component
public class PageEventHandler {

    @Bean
    public Consumer<PageEvent> pageEventConsumer() {
        return (input) -> {
            System.out.println("************");
            System.out.println(input.toString());
            System.out.println("************");
        };
    }
}
```

Chaque message reÃ§u sera affichÃ© dans la console.

ğŸ“¸ **Capture dâ€™Ã©cran â€“ Affichage des Ã©vÃ©nements dans la console :**

> ![Consumer output screenshot](images/image7.png)

---

## â±ï¸ 5. CrÃ©ation dâ€™un Supplier automatique

### ğŸ¯ Objectif

Mettre en place un **supplier** qui envoie automatiquement un Ã©vÃ©nement toutes les 200 ms dans le topic `T1`.

### ğŸ’» Code

```java
@Bean
public Supplier<PageEvent> pageEventSupplier() {
    return () -> new PageEvent(
            Math.random() > 0.5 ? "P1" : "P2",
            Math.random() > 0.5 ? "U1" : "U2",
            new Date(),
            10 + new Random().nextInt(10000)
    );
}
```

Ce `Supplier` alimente le topic de maniÃ¨re continue, simulant une activitÃ© en temps rÃ©el.

ğŸ“¸ **Capture dâ€™Ã©cran â€“ Envoi automatique vers T1 :**

> ![Supplier screenshot](images/image10.png)

---

## âš¡ 6. Traitement temps rÃ©el avec Kafka Streams

### ğŸ¯ Objectif

Appliquer un traitement de flux pour compter, toutes les 5 secondes, les Ã©vÃ©nements dont la durÃ©e est supÃ©rieure Ã  100 ms.

### ğŸ’» Code

```java
@Bean
public Function<KStream<String, PageEvent>, KStream<String, Long>> kStreamFunction() {
    return (stream) ->
            stream.filter((k, v) -> v.duration() > 100)
                    .map((k, v) -> new KeyValue<>(v.name(), 0L))
                    .groupByKey(Grouped.with(Serdes.String(), Serdes.Long()))
                    .windowedBy(TimeWindows.of(Duration.ofSeconds(5)))
                    .count()
                    .toStream()
                    .map((k, v) -> new KeyValue<>(k.key(), v));
}
```

Cette fonction permet de compter les pages actives sur des fenÃªtres temporelles glissantes.

ğŸ“¸ **Capture dâ€™Ã©cran â€“ RÃ©sultat du flux de traitement :**

> ![Stream processing screenshot](images/image12.png)

---

## ğŸ“Š 7. Visualisation temps rÃ©el des statistiques

### ğŸ¯ Objectif

Afficher en continu les rÃ©sultats du traitement via un flux SSE (`Server-Sent Events`).

### ğŸ’» Code

```java
@GetMapping(path = "/analytics", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
public Flux<Map<String, Long>> analytics() {
    return Flux.interval(Duration.ofSeconds(1))
            .map(sequence -> {
                Map<String, Long> result = new HashMap<>();
                ReadOnlyWindowStore<String, Long> windowStore =
                        interactiveQueryService.getQueryableStore("count-store", QueryableStoreTypes.windowStore());
                Instant now = Instant.now();
                Instant from = now.minusMillis(5000);
                KeyValueIterator<Windowed<String>, Long> fetchAll = windowStore.fetchAll(from, now);
                while (fetchAll.hasNext()) {
                    KeyValue<Windowed<String>, Long> next = fetchAll.next();
                    result.put(next.key.key(), next.value);
                }
                return result;
            });
}
```

AccÃ©der Ã  `http://localhost:8080/analytics` pour visualiser les statistiques mises Ã  jour en temps rÃ©el.

ğŸ“¸ **Capture dâ€™Ã©cran â€“ Visualisation des analytics :**

> ![Analytics screenshot](images/image14.png)

---

## âœ… RÃ©sultat final attendu

* Le **producer** publie rÃ©guliÃ¨rement des Ã©vÃ©nements.
* Le **consumer** les reÃ§oit et les affiche.
* Kafka Streams calcule des statistiques en temps rÃ©el.
* Lâ€™endpoint `/analytics` diffuse les donnÃ©es de maniÃ¨re continue via SSE.

ğŸ“¸ **Capture dâ€™Ã©cran â€“ RÃ©sultat final complet :**

> ![Final result screenshot](images/image15.png)

---

## ğŸ§  Configuration requise

| Composant   | Version minimale                 |
| ----------- | -------------------------------- |
| Java        | 17+                              |
| Spring Boot | 3.x                              |
| Kafka       | Local + Zookeeper                |
| Binder      | Spring Cloud Stream Kafka Binder |

---

## â–¶ï¸ Lancement du projet

### DÃ©marrage du serveur

```bash
mvn spring-boot:run
```

### Tests

* Publier un Ã©vÃ©nement :
  `http://localhost:8080/publish?name=page1&topic=T1`
* Visualiser les statistiques :
  `http://localhost:8080/analytics`

---

## ğŸ“š Ressources utiles

* [Documentation Spring Cloud Stream](https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/)
* [Documentation Kafka Streams](https://kafka.apache.org/documentation/streams/)

```
```
